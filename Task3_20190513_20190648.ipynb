{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {
        "id": "p3nSJ7-1RxlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de93f3ea-451d-42e7-f1d6-fcb87be5d64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 652
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import requests  # For making HTTP requests\n",
        "import wikipedia  # For accessing Wikipedia articles and their content\n",
        "import re  # For working with regular expressions\n",
        "import os  # For reading and writing files\n",
        "import pickle  # For save model\n",
        "from gensim.parsing.preprocessing import STOPWORDS  # A set of common stop words\n",
        "\n",
        "# Download set of common punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_documents(topic_list, num_docs):\n",
        "    \"\"\"\n",
        "    Downloads a specified number of Wikipedia articles for each topic in the given list of topics.\n",
        "\n",
        "    Args:\n",
        "        topic_list (list): A list of strings representing the topics to download articles for.\n",
        "        num_docs (int): The number of articles to download for each topic.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings representing the downloaded articles.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the downloaded documents\n",
        "    docs = []\n",
        "\n",
        "    # Loop through each topic in the list and download the specified number of articles\n",
        "    for topic in topic_list:\n",
        "        # Get the Wikipedia page object for the current topic\n",
        "        page = wikipedia.page(topic)\n",
        "        # Get the page content as a string\n",
        "        content = page.content\n",
        "        # Split the content into sections based on the \"==\" delimiter\n",
        "        sections = content.split('\\n\\n== ')\n",
        "        # Loop through the sections and add the specified number of documents to the docs list\n",
        "        for i in range(1, num_docs+1):\n",
        "            try:\n",
        "                # Get the i-th section as a string and add it to the docs list\n",
        "                doc = sections[i]\n",
        "                docs.append(doc)\n",
        "            except:\n",
        "                # If there are fewer than num_docs sections, skip the current iteration\n",
        "                pass\n",
        "\n",
        "    # Return the list of downloaded documents\n",
        "    return docs"
      ],
      "metadata": {
        "id": "tOKlefXZSERV"
      },
      "execution_count": 653,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 5 Wikipedia articles each for the topics 'Cities weather' and 'stock market' and store them in two separate lists\n",
        "weather = download_documents(['Cities weather'], 5)\n",
        "market = download_documents(['stock market'], 5)\n",
        "\n",
        "# Combine the two lists of documents into a single training list\n",
        "train_docs = weather + market\n",
        "\n",
        "# Create a list of labels for the training documents\n",
        "train_labels=['weather','weather','weather','weather','weather','market','market','market','market','market']\n",
        "\n",
        "# Download 3 more articles each for the topics 'Cities weather' and 'stock market' and store them in two separate lists\n",
        "weather = download_documents(['Cities weather'], 3)\n",
        "market = download_documents(['stock market'], 3)\n",
        "\n",
        "# Combine the two lists of test documents into a single test list\n",
        "test_docs = weather + market"
      ],
      "metadata": {
        "id": "910V1eMOSG4N"
      },
      "execution_count": 654,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def class_prob(doc_labels):\n",
        "    \"\"\"\n",
        "    Calculates the class probabilities for a given list of document labels.\n",
        "\n",
        "    Args:\n",
        "        doc_labels (list): A list of strings representing the class labels for a set of documents.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping each unique label to its corresponding class probability.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty dictionary to store the class counts for each label\n",
        "    class_counts = {}\n",
        "    # Get the total number of documents\n",
        "    total_count = len(doc_labels)\n",
        "    # Loop through each label in the list of document labels\n",
        "    for label in doc_labels:\n",
        "        # If the label is already in the class_counts dictionary, increment its count by 1\n",
        "        if label in class_counts:\n",
        "            class_counts[label] += 1\n",
        "        # Otherwise, add the label to the dictionary with a count of 1\n",
        "        else:\n",
        "            class_counts[label] = 1\n",
        "    # Initialize an empty dictionary to store the class probabilities for each label\n",
        "    class_probs = {}\n",
        "    # Loop through each label in the class_counts dictionary\n",
        "    for label in class_counts:\n",
        "        # Calculate the class probability for the current label\n",
        "        class_prob = class_counts[label] / total_count\n",
        "        # Add the label and its corresponding class probability to the class_probs dictionary\n",
        "        class_probs[label] = class_prob\n",
        "    # Return the dictionary of class probabilities\n",
        "    return class_probs"
      ],
      "metadata": {
        "id": "83CbI4XQUNUm"
      },
      "execution_count": 655,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(doc, stop_words= set(STOPWORDS)):\n",
        "    \"\"\"\n",
        "    Performs basic text preprocessing on a given document, including removing punctuation and stop words, converting to lowercase, and removing digits.\n",
        "\n",
        "    Args:\n",
        "        doc (str): A string representing the document to be preprocessed.\n",
        "        stop_words (set): An optional set of stop words to be removed from the document.\n",
        "        By default, the stop words provided by gensim are used.\n",
        "\n",
        "    Returns:\n",
        "        A preprocessed version of the input document as a string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove punctuation from the document\n",
        "    doc = re.sub(r'[^\\w\\s]', '', doc)\n",
        "\n",
        "    # Remove digits from the document\n",
        "    doc = re.sub(r'[0-9]+', '', doc)\n",
        "\n",
        "    # Convert the document to lowercase\n",
        "    doc = doc.lower()\n",
        "\n",
        "    # Split the document into words\n",
        "    words = doc.split()\n",
        "\n",
        "    # Add some custom stop words to the set\n",
        "    stop_words.update({\"ll\", \"m\", \"re\", \"s\", \"ve\" ,\"nt\",\"t\",\"us\"})\n",
        "\n",
        "    # Remove stop words and words with length less than or equal to 1 from the document\n",
        "    words = [word.strip() for word in words if (word not in stop_words) and (len(word) > 1)]\n",
        "\n",
        "    # Join the remaining words back into a single string and return the preprocessed document\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "c5kKQL2bU20R"
      },
      "execution_count": 656,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokens(doc):\n",
        "    \"\"\"\n",
        "    Tokenizes a given document by splitting it into words and returning a set of the unique words.\n",
        "\n",
        "    Args:\n",
        "        doc (str): A string representing the document to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        A set of the unique words contained in the input document.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the document into words\n",
        "    tokens = doc.split()\n",
        "\n",
        "    # Convert the list of tokens to a set to remove duplicates\n",
        "    unique_tokens = set(tokens)\n",
        "\n",
        "    # Return the set of unique tokens\n",
        "    return unique_tokens"
      ],
      "metadata": {
        "id": "kCYgwvVpWlwf"
      },
      "execution_count": 657,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_prob(preprocessed_docs, train_labels, class_name, token, vocab_size):\n",
        "    \"\"\"\n",
        "    Computes the conditional probability of a given token appearing in a document belonging to a given class.\n",
        "\n",
        "    Args:\n",
        "        preprocessed_docs (list): A list of preprocessed documents, where each document is represented as a string.\n",
        "        train_labels (list): A list of labels indicating the class of each document in preprocessed_docs.\n",
        "        class_name (str): The name of the class for which the conditional probability is being calculated.\n",
        "        token (str): The token for which the conditional probability is being calculated.\n",
        "        vocab_size (int): The size of the vocabulary (i.e., the total number of unique tokens in all documents).\n",
        "\n",
        "    Returns:\n",
        "        The conditional probability of the given token appearing in a document belonging to the given class.\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the set of all unique tokens in documents belonging to the given class\n",
        "    total_tokens = set()\n",
        "    class_tokens = 0\n",
        "    for i in range(len(preprocessed_docs)):\n",
        "        if train_labels[i] == class_name:\n",
        "            total_tokens.update(getTokens(preprocessed_docs[i]))\n",
        "\n",
        "    # Count the number of times the token appears in documents belonging to the given class\n",
        "    for i in range(len(preprocessed_docs)):\n",
        "        if train_labels[i] == class_name:\n",
        "            class_tokens += preprocessed_docs[i].count(token)\n",
        "\n",
        "    # Calculate the conditional probability\n",
        "    return (class_tokens + 1) / (len(total_tokens) + vocab_size)\n"
      ],
      "metadata": {
        "id": "Km3OuOWn2h7V"
      },
      "execution_count": 658,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(train_docs, train_labels, path='probs.pkl'):\n",
        "    \"\"\"\n",
        "    fit the data and save the probabilities.\n",
        "\n",
        "    Args:\n",
        "        train_docs (list): A list of documents, where each document is represented as a string.\n",
        "        train_labels (list): A list of labels indicating the class of each document in preprocessed_docs.\n",
        "        path (str): A string of the path to save the model\n",
        "\n",
        "    \"\"\"\n",
        "    # Compute class probabilities\n",
        "    class_probs = class_prob(train_labels)\n",
        "\n",
        "    # Preprocess training documents\n",
        "    preprocessed_docs = [preprocessing(doc) for doc in train_docs]\n",
        "\n",
        "    # Generate vocabulary of unique tokens in training data\n",
        "    train_tokens = set()\n",
        "    for doc in preprocessed_docs:\n",
        "        train_tokens.update(getTokens(doc))\n",
        "    \n",
        "    # get the total number of tokens\n",
        "    vocab_size = len(train_tokens)\n",
        "\n",
        "    # Compute conditional probabilities of each token given each class\n",
        "    class_word_probs = {}\n",
        "    for label in class_probs.keys():\n",
        "        for token in train_tokens:\n",
        "            class_word_probs[(label, token)] = conditional_prob(preprocessed_docs, train_labels, label, token, vocab_size)\n",
        "\n",
        "    # Save class probabilities and class-word probabilities to a pickle file\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump((class_probs, class_word_probs), f)\n",
        "\n",
        "# fit the model\n",
        "fit_model(train_docs, train_labels)\n"
      ],
      "metadata": {
        "id": "B1Dvxxk82h5C"
      },
      "execution_count": 659,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test_doc, model_path):\n",
        "    \"\"\"\n",
        "    load the probabilities and predict document category.\n",
        "\n",
        "    Args:\n",
        "        test_docs (str): A document represented as a string.\n",
        "        model_path (str): A string of the path to load the model\n",
        "\n",
        "    \"\"\"\n",
        "    # Load model from disk\n",
        "    with open(model_path, 'rb') as f:\n",
        "        class_prob, token_prob = pickle.load(f)\n",
        "    \n",
        "    # Preprocess test document\n",
        "    test_doc = preprocessing(test_doc)\n",
        "    \n",
        "    # Generate vocabulary of unique tokens\n",
        "    tokens = getTokens(test_doc)\n",
        "\n",
        "    # Create list of labels\n",
        "    labels = list(class_prob.keys())\n",
        "\n",
        "    # Calculate the initial score for each class\n",
        "    class_scores = class_prob[labels[0]]/class_prob[labels[1]]\n",
        "\n",
        "    # Iterate through each token in the test document\n",
        "    for token in tokens:\n",
        "        # Check if the token appears in the training data for either class\n",
        "        if ((labels[0], token) in token_prob)or((labels[1], token) in token_prob):\n",
        "            # Update the class score with the conditional probability of the token given each class\n",
        "            class_scores *= (token_prob[(labels[0], token)]/token_prob[(labels[1], token)])\n",
        "    \n",
        "    # Return class with highest score\n",
        "    if class_scores>1:\n",
        "        return(labels[0])\n",
        "    else:\n",
        "        return(labels[1])\n",
        "\n",
        "print(\"Predict of the training data :-\\n\")\n",
        "for i in train_docs:\n",
        "  print(predict(i,'probs.pkl'))\n",
        "print('------------------------------------\\n')\n",
        "print(\"Predict of the testing data :-\\n\")\n",
        "for i in test_docs:\n",
        "  print(predict(i,'probs.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQh1JNUX2h0J",
        "outputId": "4ea8815c-0608-4d6d-ead3-2b56ec06484d"
      },
      "execution_count": 663,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict of the training data:-\n",
            "\n",
            "weather\n",
            "weather\n",
            "weather\n",
            "weather\n",
            "weather\n",
            "market\n",
            "market\n",
            "market\n",
            "market\n",
            "market\n",
            "------------------------------------\n",
            "\n",
            "Predict of the testing data:-\n",
            "\n",
            "weather\n",
            "weather\n",
            "weather\n",
            "market\n",
            "market\n",
            "market\n"
          ]
        }
      ]
    }
  ]
}